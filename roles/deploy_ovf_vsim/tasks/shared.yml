##TODO:
## Assign IC MACs
## Assign unique sysids
## Add partner info to bootarg
## run node setup on node2
## run cluster setup on node1

---
- name: set credentials
  set_fact:
    esxi_login: &esxi_login
      hostname: '{{ vcenter_address }}'
      username: '{{ vcenter_username }}'
      password: '{{ vcenter_password }}'   
      validate_certs: no 

- name: parse vars
  include_tasks: parse_vars.yml

- name: set partner identifiers
  set_fact:
    node1_name:   "{{ vm_name }}"
    node2_name:   "{{ partner_name }}"
    node1_sysid:  "{{ nvram_sysid }}"
    node2_sysid:  "{{ nvram_sysid|int + 1 }}"
    node1_serial: "{{ sys_serial_number }}"
    node2_serial: "{{ partner_serial|default('4034389-06-2',true) }}"
    node1_icmac:   "{{ node1_icmac | default('02:0C:01' | random_mac ) }}"
    node2_icmac:   "{{ node2_icmac | default('02:0C:02' | random_mac ) }}"
    node2_mgmt_ip: "{{ node2_mgmt_ip | default(partner_ip|default('')) }}"
    node_setup_delay: "{{ node_setup_delay | default(120) }}"
    add_nodes_by_serial: "{{ add_nodes_by_serial | default( partner_serial|default('4034389-06-2',true),true ) }}"

- name: enforce minimum nics
  set_fact:
    vm_num_nics: 5
  when: vm_num_nics|int <= 4

# Now we can deploy the VM
- name: '{{item}}: Deploy ovf file: {{ovf_file}}' 
  community.vmware.vmware_deploy_ovf:
    <<: *esxi_login
    datacenter: '{{ vcenter_datacenter }}'
    cluster: '{{ vcenter_cluster }}'
    datastore: "{{ vm_datastore }}"
    name: "{{ item }}"
    ovf:  "{{ ovf_file }}"
    disk_provisioning: "{{vm_disk_provisioning}}"
    power_on: no
    networks: "{u'hostonly':u'{{ cluster_network }}',u'nat':u'{{ data_network }}'}"
    wait_for_ip_address: false
  delegate_to: localhost
  retries: 10
  delay: 60
  register: result           
  until: result is succeeded 
  loop:
    - "{{node1_name}}"
    - "{{node2_name}}"

- name: '{{item}}: Adjust VM Sizing'
  community.vmware.vmware_guest:
    <<: *esxi_login
    name: '{{ item }}'
    state: present
    hardware:
      memory_mb: "{{ vm_memory_mb}}"
      num_cpus: '{{ vm_num_cpus }}'
    advanced_settings:
      - key: pciHole.start
        value: 1024  # Moving the PCI hole here gives ONTAP access to more system ram
      - key: "disk.EnableUUID"
        value: "true"  # Required for vSCSI support
  delegate_to: localhost
  loop:
    - "{{node1_name}}"
    - "{{node2_name}}"

# This wierdness is because with_sequence evaluates and fails even when this task is not executed
# bug closed as a wont_fix by ansible
- name: '{{node_name}}: add nics'
  include_tasks: add_nics.yml
  vars: 
    vm_name: "{{ node_name }}"
  when: vm_num_nics|int > 4
  loop:
    - "{{node1_name}}"
    - "{{node2_name}}"
  loop_control:
    loop_var: node_name

# This wierdness is because with_sequence evaluates and fails even when this task is not executed
# bug closed as a wont_fix by ansible
# - name: 'node2: add nics'
#   include_tasks: add_nics.yml
#   vars:
#     vm_name: "{{ node2_name }}"
#   when: vm_num_nics|int > 4

# Add scsi controllers
- name: '{{item}}: Add SCSI controllers'
  community.vmware.vmware_guest_controller:
    <<: *esxi_login
    datacenter: "{{ vcenter_datacenter }}"
    name: '{{ item }}'
    controllers:
      - state: present
        type: lsilogicsas
        controller_number: 0
        bus_sharing: virtualSharing
      - state: present
        type: lsilogicsas
        controller_number: 1
        bus_sharing: virtualSharing
      - state: present
        type: lsilogicsas
        controller_number: 2
        bus_sharing: virtualSharing
      - state: present
        type: lsilogicsas
        controller_number: 3
        bus_sharing: virtualSharing
  delegate_to: localhost
  register: disk_controller_facts
  loop:
    - "{{node1_name}}"
    - "{{node2_name}}"

# Set IC MAC
- name: '{{node1_name}}: Set IC MAC'
  community.vmware.vmware_guest_network:
    <<: *esxi_login
    datacenter: '{{ vcenter_datacenter }}'
    label: "Network adapter {{vm_num_nics}}"
    network_name: "{{cluster_network}}"
    device_type: e1000
    mac_address: "{{node1_icmac}}"
    name: '{{ node1_name }}'
    state: present
  delegate_to: localhost

# Set IC MAC
- name: '{{node2_name}}: Set IC MAC'
  community.vmware.vmware_guest_network:
    <<: *esxi_login
    datacenter: '{{ vcenter_datacenter }}'
    label: "Network adapter {{vm_num_nics}}"
    network_name: "{{cluster_network}}"
    device_type: e1000
    mac_address: "{{node2_icmac}}"
    name: '{{ node2_name }}'
    state: present
  delegate_to: localhost

- name: create shared disk shelf 0
  include_tasks: share_disk.yml
  vars:
    disk_size_mb: "{{shelf0_size}}"
    node1_scsi_controller: 0
    node2_scsi_controller: 1
    unit_number: "{{item.unit_number}}"
  when: 
    - shelf0_disk_size|default("") != ""   
    - item.disk <= shelf0_disk_count|default(shelf0_qty)|int 
  loop:
      - { disk:  1, unit_number: 0 }
      - { disk:  2, unit_number: 1 }
      - { disk:  3, unit_number: 2 }
      - { disk:  4, unit_number: 3 }
      - { disk:  5, unit_number: 4 }
      - { disk:  6, unit_number: 5 }
      - { disk:  7, unit_number: 6 }
      - { disk:  8, unit_number: 8 }
      - { disk:  9, unit_number: 9 }
      - { disk: 10, unit_number: 10 }
      - { disk: 11, unit_number: 11 }
      - { disk: 12, unit_number: 12 }
      - { disk: 13, unit_number: 13 }
      - { disk: 14, unit_number: 14 }
      - { disk: 15, unit_number: 15 }

- name: create shared disk shelf 1
  include_tasks: share_disk.yml
  vars:
    disk_size_mb: "{{shelf1_size}}"
    node1_scsi_controller: 1
    node2_scsi_controller: 0
    unit_number: "{{item.unit_number}}"
  when: 
    - shelf1_disk_size|default("") != ""   
    - item.disk <= shelf1_disk_count|default(shelf1_qty)|int 
  loop:
      - { disk:  1, unit_number: 0 }
      - { disk:  2, unit_number: 1 }
      - { disk:  3, unit_number: 2 }
      - { disk:  4, unit_number: 3 }
      - { disk:  5, unit_number: 4 }
      - { disk:  6, unit_number: 5 }
      - { disk:  7, unit_number: 6 }
      - { disk:  8, unit_number: 8 }
      - { disk:  9, unit_number: 9 }
      - { disk: 10, unit_number: 10 }
      - { disk: 11, unit_number: 11 }
      - { disk: 12, unit_number: 12 }
      - { disk: 13, unit_number: 13 }
      - { disk: 14, unit_number: 14 }
      - { disk: 15, unit_number: 15 }

- name: create shared disk shelf 2
  include_tasks: share_disk.yml
  vars:
    disk_size_mb: "{{shelf2_size}}"
    node1_scsi_controller: 2
    node2_scsi_controller: 3
    unit_number: "{{item.unit_number}}"
  when: 
    - shelf2_disk_size|default("") != ""   
    - item.disk <= shelf2_disk_count|default(shelf2_qty)|int 
  loop:
      - { disk:  1, unit_number: 0 }
      - { disk:  2, unit_number: 1 }
      - { disk:  3, unit_number: 2 }
      - { disk:  4, unit_number: 3 }
      - { disk:  5, unit_number: 4 }
      - { disk:  6, unit_number: 5 }
      - { disk:  7, unit_number: 6 }
      - { disk:  8, unit_number: 8 }
      - { disk:  9, unit_number: 9 }
      - { disk: 10, unit_number: 10 }
      - { disk: 11, unit_number: 11 }
      - { disk: 12, unit_number: 12 }
      - { disk: 13, unit_number: 13 }
      - { disk: 14, unit_number: 14 }
      - { disk: 15, unit_number: 15 }

- name: create shared disk shelf 3
  include_tasks: share_disk.yml
  vars:
    disk_size_mb: "{{shelf3_size}}"
    node1_scsi_controller: 3
    node2_scsi_controller: 2
    unit_number: "{{item.unit_number}}"
  when: 
    - shelf3_disk_size|default("") != ""   
    - item.disk <= shelf3_disk_count|default(shelf3_qty)|int 
  loop:
      - { disk:  1, unit_number: 0 }
      - { disk:  2, unit_number: 1 }
      - { disk:  3, unit_number: 2 }
      - { disk:  4, unit_number: 3 }
      - { disk:  5, unit_number: 4 }
      - { disk:  6, unit_number: 5 }
      - { disk:  7, unit_number: 6 }
      - { disk:  8, unit_number: 8 }
      - { disk:  9, unit_number: 9 }
      - { disk: 10, unit_number: 10 }
      - { disk: 11, unit_number: 11 }
      - { disk: 12, unit_number: 12 }
      - { disk: 13, unit_number: 13 }
      - { disk: 14, unit_number: 14 }
      - { disk: 15, unit_number: 15 }

- name: Start Node 2
  community.vmware.vmware_guest:
    <<: *esxi_login
    name: '{{ node2_name }}'
    state: poweredon
    wait_for_ip_address: false
  delegate_to: localhost

- name: Wait for 10 seconds
  wait_for: timeout=10
  delegate_to: localhost

- name: Press Space to interrupt autoboot
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node2_name }}"
    keys_send: '{{item}}'
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: [ SPACE, SPACE, SPACE, SPACE, SPACE ]

- name: configure loader variable via sendkeys
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node2_name }}"
    keys_send: ENTER
    string_send: "{{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: 
    # Set Serial Number and Sysid
    - "setenv SYS_SERIAL_NUM {{node2_serial}}"
    - "setenv SYS_SERIAL_NUM {{node2_serial}}"
    - "setenv bootarg.nvram.sysid {{node2_sysid}}"
    # Enable dhcp for node_mgmt
    - "setenv bootarg.init.dhcp.disable false"
    # Ensure cluster lifs are auto-generated
    - "setenv bootarg.init.auto_cluster_lif.disable false"
    # Disable simulated disks
    - "setenv bootarg.vm.sim.vdevinit false"
    - "setenv bootarg.sim.vdevinit false"
    - "setenv bootarg.srm.disk.simnames false"
    - "setenv bootarg.srm.disk.simulated false"      
    # Enable virtual disks
    - "setenv bootarg.srm.disk.config.pci true"
    - "setenv bootarg.srm.virtual.adapter true"
    - "setenv bootarg.vm.data_diskmodel vscsi"  
    # Set default raid type and checksum type    
    - "setenv bootarg.vm.raidtype RAID-DP"
    - "setenv bootarg.vm.checksum_type block"
    # ADP Settings
    - "setenv root-uses-shared-disks? {{adp_enabled|default('false')}}"
    - "setenv allow-root-data1-data2-partitions? {{adp_enabled|default('false')}}"
    # Trigger an automatic option 4
    - "setenv bootarg.bootmenu.allow_opts true"
    - "setenv bootarg.bootmenu.args 4a"
    - "setenv bootarg.init.bootmenu true"
    - "setenv bootarg.vm.ha TRUE"
    - "setenv bootarg.ic_mac \"{{vm_num_nics+1}}:{{node1_icmac|default('00:50:56:3F:01:01')}}\""
    - ""

- name: enable fake-ssd
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node2_name }}"
    keys_send: ENTER
    string_send: "{{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: 
    - "setenv wafl-idedupe-allow-lowend? true"
    - "setenv allow-flash-optimized? true"
    - "setenv bootarg.init.flash_optimized true"
    - "setenv bootarg.vm.init.flash_optimized true"
    - "setenv allow-ssd-partitions? true"
    - "setenv fake-ssd {{ ((((fake_ssd_disk_size)-1/9)*8/9)|int)-6}}"
    - ""
  when: fake_ssd_disk_size|default(0) != 0

# This prevents duplicate sysids from being used when the role is called
# multiple times within a playbook's tasks section.
- name: clear nvram_sysid
  set_fact:
    nvram_sysid: ""

- name: set extra bootargs
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node2_name }}"
    keys_send: ENTER
    string_send: "set {{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: "{{bootargs}}"
  when: bootargs is defined

- name: Start Node 1
  community.vmware.vmware_guest:
    <<: *esxi_login
    name: '{{ node1_name }}'
    state: poweredon
    wait_for_ip_address: false
  delegate_to: localhost

- name: Wait for 10 seconds
  wait_for: timeout=10
  delegate_to: localhost

- name: Press Space to interrupt autoboot
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node1_name }}"
    keys_send: '{{item}}'
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: [ SPACE, SPACE, SPACE, SPACE, SPACE ]

- name: configure loader variable via sendkeys
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node1_name }}"
    keys_send: ENTER
    string_send: "{{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: 
    # Set Serial Number and Sysid
    - "setenv SYS_SERIAL_NUM {{node1_serial}}"
    - "setenv SYS_SERIAL_NUM {{node1_serial}}"
    - "setenv bootarg.nvram.sysid {{node1_sysid}}"
    # Enable dhcp for node_mgmt
    - "setenv bootarg.init.dhcp.disable false"
    # Ensure cluster lifs are auto-generated
    - "setenv bootarg.init.auto_cluster_lif.disable false"
    # Disable simulated disks
    - "setenv bootarg.vm.sim.vdevinit false"
    - "setenv bootarg.sim.vdevinit false"
    - "setenv bootarg.srm.disk.simnames false"
    - "setenv bootarg.srm.disk.simulated false"      
    # Enable virtual disks
    - "setenv bootarg.srm.disk.config.pci true"
    - "setenv bootarg.srm.virtual.adapter true"
    - "setenv bootarg.vm.data_diskmodel vscsi"  
    # Set default raid type and checksum type    
    - "setenv bootarg.vm.raidtype RAID-DP"
    - "setenv bootarg.vm.checksum_type block"
    # ADP Settings
    - "setenv root-uses-shared-disks? {{adp_enabled|default('false')}}"
    - "setenv allow-root-data1-data2-partitions? {{adp_enabled|default('false')}}"
    # Trigger an automatic option 4
    - "setenv bootarg.bootmenu.allow_opts true"
    - "setenv bootarg.bootmenu.args 4a"
    - "setenv bootarg.init.bootmenu true"
    - "setenv bootarg.vm.ha TRUE"
    - "setenv bootarg.ic_mac \"{{vm_num_nics+1}}:{{node2_icmac|default('00:50:56:3F:02:02')}}\""
    - ""

- name: enable fake-ssd
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node1_name }}"
    keys_send: ENTER
    string_send: "{{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: 
    - "setenv wafl-idedupe-allow-lowend? true"
    - "setenv allow-flash-optimized? true"
    - "setenv bootarg.init.flash_optimized true"
    - "setenv bootarg.vm.init.flash_optimized true"
    - "setenv allow-ssd-partitions? true"
    - "setenv fake-ssd {{ ((((fake_ssd_disk_size)-1/9)*8/9)|int)-6}}"
    - ""
  when: fake_ssd_disk_size|default(0) != 0

- name: boot_ontap
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node2_name }}"
    keys_send: ENTER
    string_send: "{{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: 
    - "boot_ontap"
    - "" # this is here to force the enter key after the preceding command

- name: boot_ontap
  community.vmware.vmware_guest_sendkey:
    <<: *esxi_login
    name: "{{ node1_name }}"
    keys_send: ENTER
    string_send: "{{item}}"
  delegate_to: localhost
  register: sendkeys_result
  retries: 3
  until: sendkeys_result is succeeded
  loop: 
    - "boot_ontap"
    - "" # this is here to force the enter key after the preceding command

- name: Wait for VMware tools to become available
  community.vmware.vmware_guest_tools_wait:
    <<: *esxi_login
    name: '{{ node2_name }}'
  delegate_to: localhost
  retries: 3
  delay: 15
  register: result           
  until: result is succeeded 

# # Node Setup
# - name: 'Run node setup on {{ node2_name }}'
#   block:
#   # vmware tools come up a little early so we need to pause for a bit
#   - name: 'Wait for {{ node_setup_delay|default(60) }} seconds for Startup to Complete'
#     wait_for: 'timeout={{ node_setup_delay|default(60) }}'
#     delegate_to: localhost

#   # Send keys via HID to complete setup at the vidconsole
#   - name: Complete Node Setup on node2
#     community.vmware.vmware_guest_sendkey:
#       <<: *esxi_login
#       name: "{{ node2_name }}"
#       keys_send: ENTER
#       string_send: "{{ item }}"
#     delegate_to: localhost
#     loop: 
#       - "admin"
#       - "cluster setup"
#       - "yes"
#       - "e0c" # node mgmt port
#       - "{{node2_mgmt_ip}}"
#       - "{{ontap_netmask}}"
#       - "{{ontap_gateway}}"
#       - "" # this completes the node setup phase
#   when: node2_mgmt_ip|default("") != ""

- name: run setup
  include_tasks: setup.yml



